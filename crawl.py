import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from collections import deque
import json
import time
import re

# C·∫•u h√¨nh
START_URL = "https://www.hutech.edu.vn/"
DOMAIN = "hutech.edu.vn"
MAX_PAGES = 10  # crawl t·ªëi ƒëa 500 trang

visited = set()
queue = deque([START_URL])
data = []

def is_internal(url):
    return DOMAIN in urlparse(url).netloc

def clean_text(text):
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def detect_tag(title, text):
    text = (title + " " + text).lower()
    if "h·ªçc ph√≠" in text: return "hoc_phi"
    if "ƒëƒÉng k√Ω" in text or "h·ªçc ph·∫ßn" in text: return "dang_ky_mon"
    if "th√¥ng b√°o" in text: return "thong_bao"
    if "l·ªãch h·ªçc" in text or "th·ªùi kh√≥a bi·ªÉu" in text: return "lich_hoc"
    if "ƒëi·ªÉm" in text or "k·∫øt qu·∫£ h·ªçc t·∫≠p" in text: return "diem"
    if "h·ªçc b·ªïng" in text: return "hoc_bong"
    if "tuy·ªÉn sinh" in text: return "tuyen_sinh"
    if "khoa" in text and "hutech" in text: return "gioi_thieu_khoa"
    return "khac"

# B·∫Øt ƒë·∫ßu crawl
while queue and len(visited) < MAX_PAGES:
    url = queue.popleft()
    if url in visited:
        continue

    try:
        res = requests.get(url, timeout=10)
        res.encoding = "utf-8"
        if res.status_code != 200 or "text/html" not in res.headers.get("content-type", ""):
            continue
    except Exception as e:
        print("‚ùå L·ªói khi truy c·∫≠p:", url, "->", e)
        continue

    visited.add(url)
    soup = BeautifulSoup(res.text, "html.parser")

    title = soup.title.text.strip() if soup.title else ""
    paragraphs = [clean_text(p.text) for p in soup.find_all("p") if len(p.text.strip()) > 40]
    text_content = " ".join(paragraphs)

    if title and text_content:
        tag = detect_tag(title, text_content)
        data.append({
            "tag": tag,
            "patterns": [title],
            "responses": [text_content[:400] + "..."]
        })
        print(f"‚úÖ {len(visited)}. {title[:70]}")

    for a in soup.find_all("a", href=True):
        link = urljoin(url, a['href'])
        if is_internal(link) and link not in visited and len(queue) < 1000:
            queue.append(link)

    time.sleep(0.5)

print(f"\nüåê ƒê√£ crawl {len(visited)} trang, chu·∫©n b·ªã l∆∞u file...")

# Xu·∫•t intents.json
with open("intents.json", "w", encoding="utf-8") as f:
    json.dump({"intents": data}, f, ensure_ascii=False, indent=2)

print("‚úÖ ƒê√£ l∆∞u v√†o intents.json")
